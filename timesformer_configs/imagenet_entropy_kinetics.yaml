# timesformer_configs/imagenet_entropy_kinetics.yaml

# 基本信息
dataset: kinetics200
data_path: ../mini_kinetics # 确保这是您的数据集路径
ckpt_path: ./checkpoints
exp_name: imagenet_entropy_kinetics200_exp
model_type: timesformer
al_algorithm: entropy # **关键修改: 采样策略设置为entropy**

# 数据处理
train_batch_size: 8   # TimeSformer模型较大，建议使用较小的batch size
val_batch_size: 4
workers: 16           # Kinetics数据集较大，建议增加workers
clip_len: 8           # TimeSformer通常使用8帧输入

# Active Learning 参数
initial_labeled_ratio: 0.05
budget_labels: 8000   # **关键修改: 总标注预算**
num_each_iter: 80     # **关键修改: 每轮选择的样本数**
al_train_epochs: 5   # 每个AL循环中的微调轮数

# 训练设置
train: true
seed: 42
optimizer:
  type: 'SGD'
  lr: 0.0005
  momentum: 0.9
  weight_decay: 0.0001
  nesterov: True
epoch_num: 30         # 最终收敛训练的总轮数
patience: 15          # 早停耐心

# 学习率调度器
param_scheduler:
  - type: 'MultiStepLR'
    begin: 0
    end: 30 # 匹配 epoch_num
    by_epoch: True
    milestones: [15, 25]
    gamma: 0.1

# MMACTION2 配置
model_cfg_path: ../mmaction2/configs/recognition/timesformer/timesformer_spaceOnly_8xb8-8x32x1-15e_kinetics400-rgb.py
# **关键**: 权重指向在 ImageNet 上预训练的 ViT 模型
model_ckpt_path: ../pretrained_checkpoints/vit_base_patch16_224.pth
num_classes: 200      # Kinetics-200的类别数
embed_dim: 768        # TimeSformer-Base的特征维度

# RL相关参数在此流程中不会被使用，但按模板保留
lr_dqn: 0.0001
dqn_bs: 80